{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c5684a",
   "metadata": {},
   "source": [
    "# NLP Capstone - Task 2: Exploratory Data Analysis (EDA)\n",
    "Este notebook executa uma análise exploratória sobre um corpus textual amostrado, identificando a frequência de palavras, 2-grams e 3-grams. Também examina a cobertura de vocabulário e discute estratégias para compressão semântica e ampliação de cobertura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54142b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "\n",
    "# Caminho do arquivo\n",
    "file_path = \"final/en_US/en_US.twitter.txt\"\n",
    "\n",
    "# Carregamento amostrado\n",
    "def sample_lines(file_path, sample_size=10000, prob=0.03):\n",
    "    sampled = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            if len(sampled) >= sample_size:\n",
    "                break\n",
    "            if random.random() < prob:\n",
    "                sampled.append(line.strip())\n",
    "    return sampled\n",
    "\n",
    "# Tokenização simples\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amostragem\n",
    "lines = sample_lines(file_path)\n",
    "\n",
    "# Tokenização\n",
    "tokens = []\n",
    "for line in lines:\n",
    "    tokens.extend(tokenize(line))\n",
    "\n",
    "print(f\"Total de palavras: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de palavras\n",
    "word_freq = Counter(tokens)\n",
    "common_words = word_freq.most_common(20)\n",
    "df_words = pd.DataFrame(common_words, columns=['word', 'freq'])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='freq', y='word', data=df_words)\n",
    "plt.title('Top 20 palavras mais frequentes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e62683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de n-grams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "# Frequência\n",
    "bigram_freq = Counter(bigrams).most_common(10)\n",
    "trigram_freq = Counter(trigrams).most_common(10)\n",
    "\n",
    "print(\"Top 10 bigrams:\")\n",
    "for bg in bigram_freq: print(' '.join(bg[0]), \"-\", bg[1])\n",
    "\n",
    "print(\"\\nTop 10 trigrams:\")\n",
    "for tg in trigram_freq: print(' '.join(tg[0]), \"-\", tg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004bf1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cobertura de vocabulário\n",
    "total_words = sum(word_freq.values())\n",
    "sorted_counts = sorted(word_freq.values(), reverse=True)\n",
    "cumulative = 0\n",
    "unique_words_50 = 0\n",
    "unique_words_90 = 0\n",
    "\n",
    "for i, freq in enumerate(sorted_counts):\n",
    "    cumulative += freq\n",
    "    if not unique_words_50 and cumulative >= 0.5 * total_words:\n",
    "        unique_words_50 = i + 1\n",
    "    if not unique_words_90 and cumulative >= 0.9 * total_words:\n",
    "        unique_words_90 = i + 1\n",
    "        break\n",
    "\n",
    "print(f\"Palavras únicas para cobrir 50%: {unique_words_50}\")\n",
    "print(f\"Palavras únicas para cobrir 90%: {unique_words_90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100719c",
   "metadata": {},
   "source": [
    "### Discussões e ideias:\n",
    "- Palavras muito frequentes são geralmente stopwords (ex: 'the', 'and', 'you').\n",
    "- A cobertura de vocabulário mostra que **poucas palavras** cobrem grande parte do uso.\n",
    "- Pode-se aumentar a cobertura com:\n",
    "  - Lematização e stemming\n",
    "  - Inclusão de dicionários externos (ex: WordNet)\n",
    "  - Expansão com embeddings ou subword models\n",
    "- Palavras estrangeiras podem ser detectadas com bibliotecas como `langdetect` ou listas conhecidas.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
