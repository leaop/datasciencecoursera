{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4eb8ad",
   "metadata": {},
   "source": [
    "# NLP Capstone - Task 1: Tokenização e Filtro de Profanidade (News)\n",
    "Este notebook realiza a leitura, amostragem, tokenização e limpeza de texto com remoção de palavras ofensivas usando o dataset *en_US.news.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ea09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# Caminho do arquivo\n",
    "file_path = \"final/en_US/en_US.news.txt\"\n",
    "\n",
    "# Lista de palavras ofensivas (pode ser estendida ou carregada de arquivo)\n",
    "profanity_list = [\"damn\", \"hell\", \"shit\", \"fuck\", \"bitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lines(file_path, sample_size=10000, prob=0.05):\n",
    "    sampled = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            if len(sampled) >= sample_size:\n",
    "                break\n",
    "            if random.random() < prob:\n",
    "                sampled.append(line.strip())\n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff372a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e062ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_profanity(tokens, profanity_list):\n",
    "    clean_tokens = [word for word in tokens if word not in profanity_list]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(lines, profanity_list):\n",
    "    all_tokens = []\n",
    "    for line in lines:\n",
    "        tokens = tokenize(line)\n",
    "        clean_tokens = remove_profanity(tokens, profanity_list)\n",
    "        all_tokens.extend(clean_tokens)\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39175fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amostragem das linhas\n",
    "sampled_lines = sample_lines(file_path, sample_size=10000, prob=0.03)\n",
    "\n",
    "# Processamento\n",
    "cleaned_tokens = process_text(sampled_lines, profanity_list)\n",
    "\n",
    "print(\"Exemplo de tokens limpos:\", cleaned_tokens[:30])\n",
    "print(f\"Total de palavras após limpeza: {len(cleaned_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando tokens limpos em arquivo\n",
    "output_path = \"cleaned_tokens_news.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(cleaned_tokens))\n",
    "\n",
    "print(f\"Tokens limpos salvos em: {output_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
